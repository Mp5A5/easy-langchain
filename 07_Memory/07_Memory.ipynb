{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sugarforever/wtf-langchain/blob/main/07_Memory/07_Memory.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install langchain==0.3.7\n",
    "pip install langchain-chroma==0.1.4\n",
    "pip install langchain-community==0.3.5\n",
    "pip install langchain-core==0.3.18\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langchain-ollama==0.2.0\n",
    "pip install langchain-openai==0.2.8\n",
    "pip install langchain-text-splitters==0.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "1ILWDRoJ-NmX"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Blhn900A-SI9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/0jr_vbps6_95p6zlp252byym0000gn/T/ipykernel_51467/2698402863.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory()\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferMemory()\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gcsHB8Fa_fpq",
    "outputId": "635f1dd6-9c2e-4e0c-aeba-7b77c99f4176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='whats up', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dz-T1nKS-Wa8",
    "outputId": "ca78781f-bcbb-4d9e-97df-a189e2e2e43f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: hi\\nAI: whats up'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FDXjUwRk-YkS"
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "9xbrG4Vm-cnG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [HumanMessage(content='hi', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='whats up', additional_kwargs={}, response_metadata={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "qkJ4tGlGCOgm"
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferWindowMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "vMuMG8yzCRDE"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/0jr_vbps6_95p6zlp252byym0000gn/T/ipykernel_51467/1137928133.py:1: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferWindowMemory( k=1)\n"
     ]
    }
   ],
   "source": [
    "memory = ConversationBufferWindowMemory( k=1)\n",
    "memory.save_context({\"input\": \"Hi, LangChain!\"}, {\"output\": \"Hey!\"})\n",
    "memory.save_context({\"input\": \"Where are you?\"}, {\"output\": \"By your side\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lDSZ8psOCUgn",
    "outputId": "654dc0e9-5d7b-4b58-8ca2-8124ae5d3006"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'Human: Where are you?\\nAI: By your side'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GDSbCvruCWOb",
    "outputId": "fc570f38-bcc9-4cff-806b-c463041d6477"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[HumanMessage(content='Hi, LangChain!', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='Hey!', additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content='Where are you?', additional_kwargs={}, response_metadata={}),\n",
       " AIMessage(content='By your side', additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory.chat_memory.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "I6cFKiW4vISf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w0/0jr_vbps6_95p6zlp252byym0000gn/T/ipykernel_51467/4223385939.py:19: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  llm_chain = LLMChain(\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryMemory, ChatMessageHistory\n",
    "from langchain_ollama import OllamaLLM\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "template = \"\"\"You are a chatbot having a conversation with a human.\n",
    "\n",
    "{conversation_history}\n",
    "\n",
    "Human: {input}\n",
    "Chatbot:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"conversation_history\", \"input\"], template=template\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"conversation_history\")\n",
    "\n",
    "llm_chain = LLMChain(\n",
    "    llm=OllamaLLM(temperature=0, model=\"qwen2.5:14b\"),\n",
    "    prompt=prompt,\n",
    "    verbose=True,\n",
    "    memory=memory,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 256
    },
    "id": "HYZT0RQ1l3DF",
    "outputId": "66854d0c-3b5f-4635-c0d8-8f8130009ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "\n",
      "\n",
      "Human: Where is Paris?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'Where is Paris?',\n",
       " 'conversation_history': '',\n",
       " 'text': 'Paris is the capital and most populous city of France. It is located in the northern central part of the country, situated on the Seine River. The city is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, among others.'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(input=\"Where is Paris?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "id": "fexSvRR5l8BD",
    "outputId": "c4b9fbaa-4a1f-4b9d-9ea1-4ae22c19ffcf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mYou are a chatbot having a conversation with a human.\n",
      "\n",
      "Human: Where is Paris?\n",
      "AI: Paris is the capital and most populous city of France. It is located in the northern central part of the country, situated on the Seine River. The city is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, among others.\n",
      "\n",
      "Human: What did I just ask?\n",
      "Chatbot:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What did I just ask?',\n",
       " 'conversation_history': 'Human: Where is Paris?\\nAI: Paris is the capital and most populous city of France. It is located in the northern central part of the country, situated on the Seine River. The city is known for its iconic landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, among others.',\n",
       " 'text': '��刚才问的是巴黎的位置。我刚刚回答了这个问题，说明了巴黎是法国的首都和人口最多的城市，并描述了它在法国北部中央地区的地理位置以及一些著名的地标。如果您有其他问题或需要更详细的信息，请告诉我！'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_chain.invoke(input=\"What did I just ask?\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQMnzD1ZmFkrax2yUbuaNc",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
