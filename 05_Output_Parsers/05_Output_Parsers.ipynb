{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sugarforever/wtf-langchain/blob/main/05_Output_Parsers/05_Output_Parsers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IkIusM-GD9MR"
   },
   "source": [
    "# 05 输出解析器\n",
    "\n",
    "LLM的输出为文本，但在程序中除了显示文本，可能希望获得更结构化的数据。这就是输出解析器（Output Parsers）的用武之地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ceq3MMqkDutF",
    "outputId": "4c479341-e009-4997-cd66-f543d41df4e7"
   },
   "outputs": [],
   "source": [
    "pip install langchain==0.3.7\n",
    "pip install langchain-chroma==0.1.4\n",
    "pip install langchain-community==0.3.5\n",
    "pip install langchain-core==0.3.18\n",
    "pip install langchain-huggingface==0.1.2\n",
    "pip install langchain-ollama==0.2.0\n",
    "pip install langchain-openai==0.2.8\n",
    "pip install langchain-text-splitters==0.3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0vNCtPT4zC6"
   },
   "source": [
    "## List Parser\n",
    "\n",
    "List Parser将逗号分隔的文本解析为列表。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUQ6R0V740yX",
    "outputId": "0adbbd93-a090-4770-e17b-31d58e5c4766"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['black', 'yellow', 'red', 'green', 'white', 'blue']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import CommaSeparatedListOutputParser\n",
    "\n",
    "output_parser = CommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"black, yellow, red, green, white, blue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KK4suFQr468t"
   },
   "source": [
    "## Structured Output Parser\n",
    "\n",
    "当我们想要类似JSON数据结构，包含多个字段时，可以使用这个输出解析器。该解析器可以生成指令帮助LLM返回结构化数据文本，同时完成文本到结构化数据的解析工作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import StructuredOutputParser, ResponseSchema\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate, HumanMessagePromptTemplate\n",
    "from langchain_openai import OpenAI, ChatOpenAI\n",
    "from langchain_ollama import OllamaLLM, ChatOllama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"回答\": string  // 回答用户的问题\\n\\t\"来源\": string  // 所提及的回答用户问题的来源，必须是一个网站。\\n}\\n```'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 定义响应的结构(JSON)，两个字段 回答和 来源。\n",
    "response_schemas = [\n",
    "    ResponseSchema(name=\"回答\", description=\"回答用户的问题\"),\n",
    "    ResponseSchema(name=\"来源\", description=\"所提及的回答用户问题的来源，必须是一个网站。\")\n",
    "]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "# 获取响应格式化的指令\n",
    "format_instructions = output_parser.get_format_instructions()\n",
    "format_instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai 聊天模型中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_model = ChatOpenAI(temperature=0, model_name=\"qwen2.5:14b\", openai_api_key='随便写都可以', openai_api_base='http://localhost:11434/v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[HumanMessage(content='尽可能回答用户的问题。\\nThe output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\\n\\n```json\\n{\\n\\t\"回答\": string  // 回答用户的问题\\n\\t\"来源\": string  // 所提及的回答用户问题的来源，必须是一个网站。\\n}\\n```\\nwhat\\'s the capital of france?', additional_kwargs={}, response_metadata={})]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' ```json\\n{\\n\\t\"回答\": \"The capital of France is Paris.\",\\n\\t\"来源\": \"https://www.britannica.com/place/Paris\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 112, 'total_tokens': 148, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5:14b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-8a544e16-42af-47ad-817f-16a632a7f96e-0', usage_metadata={'input_tokens': 112, 'output_tokens': 36, 'total_tokens': 148, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_prompt = ChatPromptTemplate(\n",
    "    messages=[\n",
    "        HumanMessagePromptTemplate.from_template(\"尽可能回答用户的问题。\\n{format_instructions}\\n{question}\")  \n",
    "    ],\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")\n",
    "\n",
    "_input = chat_prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "print(_input)\n",
    "output = chat_model.invoke(_input.to_messages())\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'回答': 'The capital of France is Paris.',\n",
       " '来源': 'https://www.britannica.com/place/Paris'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = chat_prompt.format_prompt(question=\"Tesla的CEO是谁？\")\n",
    "output = chat_model.invoke(_input.to_messages())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'回答': '特斯拉（Tesla）的首席执行官是埃隆·马斯克（Elon Musk）。', '来源': 'https://www.tesla.com/'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### openai语言模型中使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "这里使用 qwen 的模型会报错，但是使用 openai 的模型不会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAI(\n",
    "    openai_api_base = 'http://localhost:11434/v1',\n",
    "    openai_api_key = '随便写都可以',\n",
    "    model_name = 'qwen2.5:14b'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = PromptTemplate(\n",
    "    template=\"尽可能回答用户的问题。\\n{format_instructions}\\n{question}\",\n",
    "    input_variables=[\"question\"],\n",
    "    partial_variables={\"format_instructions\": format_instructions}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = prompt.format_prompt(question=\"法国的首都是哪里？\")\n",
    "output = model.invoke(response.to_string())\n",
    "output_parser.parse(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ollama 聊天模型使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=' ```json\\n{\\n\\t\"回答\": \"The capital of France is Paris.\",\\n\\t\"来源\": \"https://www.britannica.com/place/Paris\"\\n}\\n```', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 36, 'prompt_tokens': 112, 'total_tokens': 148, 'completion_tokens_details': None, 'prompt_tokens_details': None}, 'model_name': 'qwen2.5:14b', 'system_fingerprint': 'fp_ollama', 'finish_reason': 'stop', 'logprobs': None}, id='run-c9027597-d53d-46f4-8b4b-8278303628ba-0', usage_metadata={'input_tokens': 112, 'output_tokens': 36, 'total_tokens': 148, 'input_token_details': {}, 'output_token_details': {}})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ChatOllama(temperature=0, model=\"qwen2.5:14b\")\n",
    "_input = chat_prompt.format_prompt(question=\"what's the capital of france?\")\n",
    "output = chat_model.invoke(_input.to_messages())\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'回答': 'The capital of France is Paris.',\n",
       " '来源': 'https://www.britannica.com/place/Paris'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(output.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7qYA8JPv6Pnz"
   },
   "source": [
    "## 自定义输出解析器\n",
    "\n",
    "扩展CommaSeparatedListOutputParser，让其返回的列表是经过排序的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6M-py_2C6beo",
    "outputId": "922875ac-c93e-4ee6-fad0-74c052bd0b68"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "class SortedCommaSeparatedListOutputParser(CommaSeparatedListOutputParser):\n",
    "  def parse(self, text: str) -> List[str]:\n",
    "    lst = super().parse(text)\n",
    "    return sorted(lst)\n",
    "\n",
    "output_parser = SortedCommaSeparatedListOutputParser()\n",
    "output_parser.parse(\"black, yellow, red, green, white, blue\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMUPkO2WTAkNP76DCPwLqi1",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "langchain"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
